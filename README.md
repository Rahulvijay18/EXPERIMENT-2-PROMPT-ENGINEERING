# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

## Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.

Experiment

We test and compare how models respond to:

Broad / Unstructured Prompts (vague, general queries).

Refined / Structured Prompts (clear, specific, detailed queries).

The analysis is done across multiple scenarios (Q&A, coding, summarization, creative writing), focusing on:

Quality of response

Accuracy

Depth of explanation

## Algorithm

Start

Select different prompting styles:

Unstructured (broad, open-ended)

Structured (clear, step-by-step, refined)

Choose multiple test scenarios: Q&A, code generation, summarization, reasoning.

Provide the same task using both prompt styles.

Collect responses generated by the model.

Evaluate on three parameters: quality, accuracy, depth.

Compare and record observations.

Document differences in response effectiveness.

End

## Output

### Test Scenarios & Responses

| **Prompt Type**           | **Example Prompt**                                                                           | **Test Scenario**     | **Observed Response Quality**                      |
| ------------------------- | -------------------------------------------------------------------------------------------- | --------------------- | -------------------------------------------------- |
| **Zero-shot (Broad)**     | "Tell me about Climate Change."                                                              | General Knowledge     | Generic, lengthy, lacks focus.                     |
| **Zero-shot (Refined)**   | "Explain Climate Change in 4 bullet points suitable for school kids."                        | General Knowledge     | Concise, structured, audience-specific.            |
| **Few-shot**              | "Show me 2 examples of synonyms, then give me 2 for ‘Happy’."                                | Language Learning     | More structured, follows the given pattern.        |
| **Chain-of-thought**      | "Step-by-step, explain how photosynthesis converts sunlight to energy."                      | Reasoning             | Logical flow, detailed explanation.                |
| **Role-based**            | "You are a doctor. Explain diabetes management to a patient."                                | Teaching              | Professional tone, simple terms, practical advice. |
| **Instruction + Context** | "Given this paragraph on AI in healthcare, summarize in 3 points focusing only on benefits." | Context Summarization | Relevant, focused on required aspects.             |


## Result

Broad/unstructured prompts → often lead to vague, lengthy, or irrelevant answers.

Refined/structured prompts → produce focused, accurate, and deeper responses.

The quality, accuracy, and depth of output significantly improves when prompts are clear, specific, and well-structured.

Thus, the experiment demonstrates that effective prompt design (prompt engineering) is crucial to achieve high-quality results from Generative AI models.
